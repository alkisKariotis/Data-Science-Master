{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5_2 - Neural Networks\n",
    "\n",
    "Learning contents:\n",
    "1. Detailed example: Two-layer MLP for regression\n",
    "    - Forward pass: Calculate the values of $z_1$, $z_2$, and $y$\n",
    "    - Compute the mean squared error\n",
    "    - Using backpropagation, compute the gradient or the error w.r.t the weights $w^{(2)}_2$ and $w^{(1)}_{2,2}$ \n",
    "    - Compute the updated weights for $w^{(2)}_2$ and $w^{(1)}_{2,2}$ \n",
    "2. PyTorch: 2-layer MLP for classification\n",
    "    - Create and train a 2-layer MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Detailed example: Two-layer MLP for regression\n",
    "We'll be working through a forward and back-propagation example in all its details for a 2-layer MLP for regression. Our network has the following structure:\n",
    "\n",
    "![](two-layer-nn.svg)\n",
    "\n",
    "Where \n",
    "    \\begin{equation*}\n",
    "    z_j = \\text{ReLU}\\left(a_j \\right)\n",
    "    \\qquad\n",
    "    a_j = \\sum_i w^{(1)}_{ij} x_i\n",
    "    \\qquad\n",
    "    y_j = \\sum_i w^{(2)}_{ij} z_i\n",
    "    \\end{equation*}\n",
    "and the biases \n",
    "    \\begin{equation*}\n",
    "    x_0 = z_0 = 1\n",
    "    \\end{equation*}\n",
    "\n",
    "Suppose, we have the weights\n",
    "    \\begin{equation*}\n",
    "    \\mathbf{W}^{(1)} = \\left[ \n",
    "        \\begin{matrix}\n",
    "        0.1 & 0.2\\\\\n",
    "        -1.1 & 1.2\\\\\n",
    "        -2.1 & 2.2\n",
    "        \\end{matrix}\n",
    "    \\right]\n",
    "    \\qquad\n",
    "    \\mathbf{w}^{(2)} = \\left[ \n",
    "        \\begin{matrix}\n",
    "        -0.1\\\\\n",
    "        1.1 \\\\\n",
    "        2.2\n",
    "        \\end{matrix}\n",
    "    \\right]\n",
    "    \\end{equation*}\n",
    "Notice that the bias weights are included in the weight matrix. Relating it to the drawing, we have $w_{0,1} = 0.1$, $w_{1,2} = 1.2$\n",
    "\n",
    "Moreover, we are given an input\n",
    "    \\begin{equation*}\n",
    "    \\mathbf{x} = \\left[ \n",
    "        \\begin{matrix}\n",
    "        0.1 \\\\\n",
    "        0.2\n",
    "        \\end{matrix}\n",
    "    \\right]\n",
    "    \\end{equation*}\n",
    "    \n",
    "Relating these to the drawing, we have $w^{(1)}_{1,2} = 1.2$ and $x_1 = 0.1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Forward pass: Calculate the values of $z_1$, $z_2$, and $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x=torch.tensor([[0.1],[0.2]])\n",
    "\n",
    "W1=torch.tensor([\n",
    "    [0.1,0.2],\n",
    "    [-1.1,1.2],\n",
    "    [-2.1,2.2]\n",
    "], requires_grad=True\n",
    ")\n",
    "\n",
    "w2=torch.tensor([\n",
    "    [-0.1],\n",
    "    [1.1],\n",
    "    [2.1]\n",
    "], requires_grad=True\n",
    ")\n",
    "\n",
    "def augment(x):\n",
    "    return torch.cat([torch.ones(1).unsqueeze(0),x])\n",
    "\n",
    "    augment(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=tensor([[-0.4300],\n",
      "        [ 0.7600]], grad_fn=<MmBackward0>)\n",
      "z=tensor([[0.0000],\n",
      "        [0.7600]], grad_fn=<ReluBackward0>)\n",
      "y=tensor([[1.4960]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a=W1.t().mm(augment(x))\n",
    "print(f\"a={a}\")\n",
    "\n",
    "z=torch.nn.functional.relu(a)\n",
    "print(f\"z={z}\")\n",
    "\n",
    "y=w2.t().mm(augment(z))\n",
    "print(f\"y={y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Compute the mean squared error\n",
    "Suppose our target $t=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1270]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=torch.tensor([2])\n",
    "E=0.5*(y-t)**2\n",
    "E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Using backpropagation, compute the gradient or the error w.r.t the weights $w^{(2)}_2$ and $w^{(1)}_{2,2}$ \n",
    "This corresponds to the bottom row of weights on the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dEdw^2_1=tensor([[-0.3830]], grad_fn=<MulBackward0>)\n",
      "dEdw^2_2=tensor([[-0.2117]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dEdy= y-t\n",
    "dydw2_2= z[1]\n",
    "\n",
    "dEdw2_2 = dEdy*dydw2_2\n",
    "print(f\"dEdw^{(2)}_1={dEdw2_2}\")\n",
    "\n",
    "dydz2=w2[2]\n",
    "\n",
    "dz2da2=1\n",
    "\n",
    "da2w1_22=x[1]\n",
    "\n",
    "dEw1_22=dEdy*dydz2*dz2da2*da2w1_22\n",
    "print(f\"dEdw^2_2={dEw1_22}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4) Compute the updated weights for $w^{(2)}_2$ and $w^{(1)}_{2,2}$ \n",
    "Use a learning rate $\\eta = 0.1$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mE\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdEdW^1 = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mW1\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdEdw^2=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw2\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "E.backward()\n",
    "print(f\"dEdW^1 = {W1.grad}\")\n",
    "print(f\"dEdw^2={w2.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) PyTorch: 2-layer MLP for classification\n",
    "Luckily, there are some nice Deep Learning libraries out there, that make working with neural networks a pleasure.\n",
    "The two most noteable are [Tensorflow](https://www.tensorflow.org) and [PyTorch](https://pytorch.org). We'll be using the latter.\n",
    "\n",
    "In order to install it in your conda environment you can use\n",
    "```pip install torch torchvision```\n",
    "\n",
    "A key feature of these libraries is that they can handle the gradient computation for you.\n",
    "Moreover, they have a lot of layer types and losses, that are easily composable to handle computation of complex neural networks.\n",
    "\n",
    "We'll be working with the classic MNIST dataset, which we can easily get via PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 9912422/9912422 [00:00<00:00, 11656393.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 28881/28881 [00:00<00:00, 53387260.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 1648877/1648877 [00:00<00:00, 10098152.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 4542/4542 [00:00<00:00, 10690532.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_data = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST('../data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size = 64,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size = 64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few steps happened here:\n",
    "1. The dataset (train and test) was downloaded \n",
    "1. We created a `DataLoader` for each data split. Using this, we get batches of data (64 examples per batch here)\n",
    "1. We told asked for the training data to be shuffled\n",
    "\n",
    "Lets see what we get in a batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, targets = next(iter(train_loader))\n",
    "data.shape, targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(data[i][0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(targets[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Create and train a 2-layer MLP\n",
    "For the network, use a single hidden layer of 512 neurons with a ReLU activation function for the first weight Linear layer. The output of the second Linear layer should be a softmax.\n",
    "\n",
    "For optimisation, use the SGD optimizer with learning rate of 0.001, and the negative log-likelihood loss.\n",
    "\n",
    "Train the network for 5 epochs on the train data, and report the prediction accuracy on the test data. You should be able to get about 90% correct.\n",
    "\n",
    "Hint: check the [PyTorch documentation](https://pytorch.org/docs/stable/index.html) for usage of the layers, optimizers and losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear, Module\n",
    "from torch.nn.functional import relu, log_softmax, nll_loss\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "class Net(Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1=Linear(28*28,512)\n",
    "        self.fc2=Linear(512,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=torch.flatten(x,1)\n",
    "        x=relu(self.fc1(x))\n",
    "        x=self.fc2(x)\n",
    "        output=log_softmax(x,dim=1)\n",
    "        return output\n",
    "\n",
    "def train(\n",
    "    model:Module, \n",
    "    train_loader:DataLoader, \n",
    "    optimizer: SGD, \n",
    "    epoch:int, \n",
    "    log_interval = 50\n",
    "):\n",
    "    # Set model to train mode\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Reset the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Feed the data through the model\n",
    "        output=model(data)\n",
    "        \n",
    "        # Compute the negative log-likelihood loss\n",
    "        loss=nll_loss(output,target)\n",
    "        \n",
    "        # Backward propagate the gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Perform an update step using the optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            # Log (Optional)\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, \n",
    "                batch_idx * len(data), \n",
    "                len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item()\n",
    "            ))\n",
    "\n",
    "def test(model:Module, test_loader:DataLoader):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    test_loss=0\n",
    "    correct = 0\n",
    "    \n",
    "    # Don't accumulate gradients\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # Feed the data through the model\n",
    "            output=model(data)\n",
    "            \n",
    "            # Predict the class (it is the index of the max log-probability)\n",
    "            test_loss+=nll_loss(output,target,reduction='sum').item()\n",
    "            pred=output.argmax(dim=1,keepdim=True)\n",
    "            \n",
    "            # Add to the number of correct\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    # Print results\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, \n",
    "        correct, \n",
    "        len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.294124\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.224131\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.061008\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.029338\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.907256\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.817905\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.691952\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 1.686073\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.448097\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 1.509563\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.530406\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 1.356244\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.353982\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 1.190563\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.250089\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 1.093346\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.290994\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.967198\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.960568\n",
      "\n",
      "Test set: Average loss: 0.9540, Accuracy: 8248/10000 (82%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.021443\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.991096\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.844548\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.887817\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.830057\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.761067\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.812304\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.794372\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.687280\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.799896\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.683897\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.784571\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.678102\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.647985\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.673829\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.656137\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.708049\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.480619\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.493063\n",
      "\n",
      "Test set: Average loss: 0.5948, Accuracy: 8706/10000 (87%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.605092\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.474521\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.497558\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.615083\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.579036\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.630828\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.610658\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.531876\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.595898\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.533646\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.559651\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.563082\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.466606\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.561064\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.483805\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.704215\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.575673\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.471249\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.388199\n",
      "\n",
      "Test set: Average loss: 0.4756, Accuracy: 8833/10000 (88%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.554312\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.556883\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.461722\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.518217\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.359709\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.488661\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.439147\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.542486\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.421083\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.426927\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.478962\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.506725\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.662086\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.432258\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.413671\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.451720\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.406480\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.392280\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.355143\n",
      "\n",
      "Test set: Average loss: 0.4171, Accuracy: 8927/10000 (89%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.508416\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.386203\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.374348\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.447852\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.482772\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.399473\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.629529\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.330763\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.644341\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.547831\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.530323\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.292953\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.286812\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.453147\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.522922\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.250508\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.335841\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.380370\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.544118\n",
      "\n",
      "Test set: Average loss: 0.3816, Accuracy: 8973/10000 (90%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "optimizer = SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, train_loader, optimizer, epoch, log_interval=50)\n",
    "    test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
