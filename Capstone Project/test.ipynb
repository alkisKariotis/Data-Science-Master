{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "<font size=\"+3\">Time-series Generative Adversarial Network (TimeGAN)</font>\n",
    "\n",
    "# Imports & Settings\n",
    "\n",
    "Adapted from the excellent paper by Jinsung Yoon, Daniel Jarrett, and Mihaela van der Schaar:  \n",
    "[Time-series Generative Adversarial Networks](https://papers.nips.cc/paper/8789-time-series-generative-adversarial-networks),  \n",
    "Neural Information Processing Systems (NeurIPS), 2019.\n",
    "\n",
    "- Last updated Date: April 24th 2020\n",
    "- [Original code](https://bitbucket.org/mvdschaar/mlforhealthlabpub/src/master/alg/timegan/) author: Jinsung Yoon (jsyoon0823@gmail.com)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "!pip install tsaug\n",
    "!pip install fredapi\n",
    "!pip install sktime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from tsaug import TimeWarp, Crop, Quantize, Drift, Reverse\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import GRU, Dense, RNN, GRUCell, Input, Dropout, BatchNormalization, LeakyReLU, Reshape, Conv1D, Flatten, Conv1DTranspose\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, MeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.metrics import AUC\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from fredapi import Fred\n",
    "from sktime.utils.plotting import plot_series\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    print('Using GPU')\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "else:\n",
    "    print('Using CPU')\n",
    "\n",
    "sns.set_style('white')\n",
    "\n",
    "# Experiment Path\n",
    "\n",
    "results_path = Path('time_gan')\n",
    "if not results_path.exists():\n",
    "    results_path.mkdir()\n",
    "\n",
    "experiment = 0\n",
    "\n",
    "log_dir = results_path / f'experiment_{experiment:02}'\n",
    "if not log_dir.exists():\n",
    "    log_dir.mkdir(parents=True)\n",
    "\n",
    "hdf_store = results_path / 'TimeSeriesGAN.h5'\n",
    "\n",
    "# Prepare Data\n",
    "\n",
    "fred = Fred(api_key='18ecafa4ff3f8087c46dd862605532f1')\n",
    "data = fred.get_series('SP500')\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"WTI\": fred.get_series(\"DCOILWTICO\", observation_start=\"1987-05-20\", observation_end=\"2020-12-31\"),\n",
    "    \"BRENT\": fred.get_series(\"DCOILBRENTEU\", observation_start=\"1987-05-20\", observation_end=\"2020-12-31\")\n",
    "})\n",
    "df.head()\n",
    "\n",
    "df.info()\n",
    "\n",
    "df = df.dropna()\n",
    "df.info()\n",
    "\n",
    "df['WTI'].plot();\n",
    "\n",
    "df['BRENT'].plot();\n",
    "\n",
    "# Select the 'Close' column\n",
    "close_col = df['WTI']\n",
    "\n",
    "# Divide the 'Close' column by the first value in the column\n",
    "normalized_close = close_col.div(close_col.iloc[0])\n",
    "\n",
    "# Plot the normalized 'Close' column\n",
    "ax = normalized_close.plot(figsize=(14, 6), title=\"Normalized Closing Price\", legend=False, color='k')\n",
    "ax.set_xlabel('')\n",
    "\n",
    "## Plot Series\n",
    "\n",
    "## Correlation\n",
    "\n",
    "sns.clustermap(df.corr(),\n",
    "               annot=True,\n",
    "               fmt='.2f',\n",
    "               cmap=sns.diverging_palette(h_neg=20,\n",
    "                                          h_pos=220), center=0);\n",
    "\n",
    "def augment_data(original_data, augmentations):\n",
    "    augmented_data = original_data.copy()\n",
    "    augmented_data['WTI'] = augmentations.augment(original_data['WTI'].values)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df.index, df['WTI'], label='Original Data', marker='o')\n",
    "    plt.plot(augmented_data.index, augmented_data['WTI'], label='Augmented Data', linestyle='--', marker='x')\n",
    "    plt.title('Original vs Augmented Bitcoin Close Prices')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Close Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return augmented_data\n",
    "\n",
    "augmenter = (\n",
    "    TimeWarp()\n",
    "    + Crop(size=8402)\n",
    "    + Quantize(n_levels=[10, 20, 30])\n",
    "    + Drift(max_drift=(0.1, 0.5))\n",
    "    #+ Reverse()\n",
    ")\n",
    "\n",
    "augmented_data = augment_data(df, augmenter)\n",
    "\n",
    "augmenter = (\n",
    "    TimeWarp()\n",
    "    + Crop(size=8402)\n",
    "    + Quantize(n_levels=[10, 20, 30])\n",
    "    + Drift(max_drift=(0.1, 0.5))\n",
    "    + Reverse()\n",
    ")\n",
    "\n",
    "augmented_data = augment_data(df, augmenter)\n",
    "\n",
    "## Normalize Data\n",
    "\n",
    "columns_for_autoencoder = ['WTI', 'BRENT']\n",
    "scaler = MinMaxScaler()\n",
    "original_data_scaled = scaler.fit_transform(df[columns_for_autoencoder])\n",
    "\n",
    "\n",
    "input_dim = len(columns_for_autoencoder)\n",
    "encoding_dim = 80\n",
    "timesteps = original_data_scaled.shape[0]\n",
    "\n",
    "original_data_scaled = np.reshape(original_data_scaled, (timesteps, input_dim, 1))\n",
    "\n",
    "autoencoder = Sequential()\n",
    "\n",
    "# Encoder\n",
    "autoencoder.add(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', input_shape=(input_dim, 1)))\n",
    "autoencoder.add(BatchNormalization())\n",
    "autoencoder.add(Dropout(0.6))\n",
    "\n",
    "autoencoder.add(Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'))\n",
    "autoencoder.add(BatchNormalization())\n",
    "autoencoder.add(Dropout(0.6))\n",
    "\n",
    "autoencoder.add(Flatten())\n",
    "autoencoder.add(Dense(encoding_dim, activation='relu'))\n",
    "\n",
    "# Decoder\n",
    "autoencoder.add(Dense(32 * input_dim, activation='relu'))\n",
    "autoencoder.add(Reshape((input_dim, 32)))\n",
    "\n",
    "autoencoder.add(Conv1DTranspose(filters=64, kernel_size=3, activation='relu', padding='same'))\n",
    "autoencoder.add(BatchNormalization())\n",
    "autoencoder.add(Dropout(0.6))\n",
    "\n",
    "autoencoder.add(Conv1DTranspose(filters=1, kernel_size=3, activation='sigmoid', padding='same'))\n",
    "\n",
    "original_data_scaled = np.reshape(original_data_scaled, (timesteps, input_dim))\n",
    "\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.0005), loss='mean_squared_error')\n",
    "\n",
    "autoencoder.fit(original_data_scaled, original_data_scaled, epochs=20, batch_size=32, shuffle=True, validation_split=0.2)\n",
    "\n",
    "synthetic_data_scaled = autoencoder.predict(original_data_scaled)\n",
    "synthetic_data = pd.DataFrame(data=scaler.inverse_transform(synthetic_data_scaled.reshape(timesteps, input_dim)), index=df.index, columns=columns_for_autoencoder)\n",
    "\n",
    "synthetic_data = np.maximum(synthetic_data, 0)\n",
    "\n",
    "table = pd.concat([df[columns_for_autoencoder], synthetic_data.rename(columns=lambda x: 'Synthetic ' + x)], axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(table['BRENT'], label='Original Close', marker='o')\n",
    "plt.plot(table['Synthetic BRENT'], label='Synthetic Close', marker='o')\n",
    "plt.title('Original vs Synthetic Close')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(table['WTI'], label='Original Close', marker='o')\n",
    "plt.plot(table['Synthetic WTI'], label='Synthetic Close', marker='o')\n",
    "plt.title('Original vs Synthetic Close')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Time Gan\n",
    "\n",
    "## Parameters\n",
    "\n",
    "seq_len = 24\n",
    "n_seq = 2\n",
    "batch_size = 128\n",
    "\n",
    "## Create rolling window sequences\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df).astype(np.float32)\n",
    "\n",
    "data = []\n",
    "for i in range(len(df) - seq_len):\n",
    "    data.append(scaled_data[i:i + seq_len])\n",
    "\n",
    "n_windows = len(data)\n",
    "\n",
    "scaled_data\n",
    "\n",
    "## Create tf.data.Dataset\n",
    "\n",
    "real_series = (tf.data.Dataset\n",
    "               .from_tensor_slices(data)\n",
    "               .shuffle(buffer_size=n_windows)\n",
    "               .batch(batch_size))\n",
    "real_series_iter = iter(real_series.repeat())\n",
    "\n",
    "## Set up random series generator\n",
    "\n",
    "def make_random_data():\n",
    "    while True:\n",
    "        yield np.random.uniform(low=0, high=1, size=(seq_len, n_seq))\n",
    "\n",
    "We use the Python generator to feed a `tf.data.Dataset` that continues to call the random number generator as long as necessary and produces the desired batch size.\n",
    "\n",
    "random_series = iter(tf.data.Dataset\n",
    "                     .from_generator(make_random_data, output_types=tf.float32)\n",
    "                     .batch(batch_size)\n",
    "                     .repeat())\n",
    "\n",
    "# TimeGAN Components\n",
    "\n",
    "The design of the TimeGAN components follows the author's sample code.\n",
    "\n",
    "##  Network Parameters\n",
    "\n",
    "hidden_dim = 24\n",
    "num_layers = 3\n",
    "\n",
    "## Set up logger\n",
    "\n",
    "writer = tf.summary.create_file_writer(log_dir.as_posix())\n",
    "\n",
    "## Input place holders\n",
    "\n",
    "X = Input(shape=[seq_len, n_seq], name='RealData')\n",
    "Z = Input(shape=[seq_len, n_seq], name='RandomData')\n",
    "\n",
    "## RNN block generator\n",
    "\n",
    "We keep it very simple and use a very similar architecture for all four components. For a real-world application, they should be tailored to the data.\n",
    "\n",
    "def make_rnn(n_layers, hidden_units, output_units, name):\n",
    "    return Sequential([GRU(units=hidden_units,\n",
    "                           return_sequences=True,\n",
    "                           name=f'GRU_{i + 1}') for i in range(n_layers)] +\n",
    "                      [Dense(units=output_units,\n",
    "                             activation='sigmoid',\n",
    "                             name='OUT')], name=name)\n",
    "\n",
    "## Embedder & Recovery\n",
    "\n",
    "embedder = make_rnn(n_layers=3, \n",
    "                    hidden_units=hidden_dim, \n",
    "                    output_units=hidden_dim, \n",
    "                    name='Embedder')\n",
    "recovery = make_rnn(n_layers=3, \n",
    "                    hidden_units=hidden_dim, \n",
    "                    output_units=n_seq, \n",
    "                    name='Recovery')\n",
    "\n",
    "## Generator & Discriminator\n",
    "\n",
    "generator = make_rnn(n_layers=3, \n",
    "                     hidden_units=hidden_dim, \n",
    "                     output_units=hidden_dim, \n",
    "                     name='Generator')\n",
    "discriminator = make_rnn(n_layers=3, \n",
    "                         hidden_units=hidden_dim, \n",
    "                         output_units=1, \n",
    "                         name='Discriminator')\n",
    "supervisor = make_rnn(n_layers=2, \n",
    "                      hidden_units=hidden_dim, \n",
    "                      output_units=hidden_dim, \n",
    "                      name='Supervisor')\n",
    "\n",
    "# Define the Encoder\n",
    "def build_encoder(input_shape, latent_dim):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=input_shape))\n",
    "    model.add(layers.Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(latent_dim, activation=None))  # latent_dim is the size of the latent space\n",
    "    return model\n",
    "\n",
    "# Define the Decoder\n",
    "def build_decoder(output_shape, latent_dim):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=(latent_dim,)))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(output_shape[0] * output_shape[1], activation='relu'))\n",
    "    model.add(layers.Reshape((output_shape[0], output_shape[1])))\n",
    "    model.add(layers.Conv1DTranspose(64, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "    model.add(layers.Conv1DTranspose(1, kernel_size=5, strides=1, padding='same', activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# Instantiate the encoder and decoder\n",
    "input_shape = (seq_len, n_seq)\n",
    "latent_dim = 20\n",
    "encoder = build_encoder(input_shape, latent_dim)\n",
    "decoder = build_decoder(input_shape, latent_dim)\n",
    "\n",
    "# TimeGAN Training\n",
    "\n",
    "## Settings\n",
    "\n",
    "train_steps = 5000\n",
    "gamma = 1\n",
    "\n",
    "## Generic Loss Functions\n",
    "\n",
    "mse = MeanSquaredError()\n",
    "bce = BinaryCrossentropy()\n",
    "\n",
    "# Phase 1: Autoencoder Training\n",
    "\n",
    "## Architecture\n",
    "\n",
    "H = embedder(X)\n",
    "X_tilde = recovery(H)\n",
    "\n",
    "autoencoder = Model(inputs=X,\n",
    "                    outputs=X_tilde,\n",
    "                    name='Autoencoder')\n",
    "\n",
    "autoencoder.summary()\n",
    "\n",
    "plot_model(autoencoder,\n",
    "           to_file=(results_path / 'autoencoder.png').as_posix(),\n",
    "           show_shapes=True)\n",
    "\n",
    "## Autoencoder Optimizer\n",
    "\n",
    "autoencoder_optimizer = Adam()\n",
    "\n",
    "## Autoencoder Training Step\n",
    "\n",
    "@tf.function\n",
    "def train_autoencoder_init(x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        x_tilde = autoencoder(x)\n",
    "        embedding_loss_t0 = mse(x, x_tilde)\n",
    "        e_loss_0 = 10 * tf.sqrt(embedding_loss_t0)\n",
    "\n",
    "    var_list = embedder.trainable_variables + recovery.trainable_variables\n",
    "    gradients = tape.gradient(e_loss_0, var_list)\n",
    "    autoencoder_optimizer.apply_gradients(zip(gradients, var_list))\n",
    "    return tf.sqrt(embedding_loss_t0)\n",
    "\n",
    "## Autoencoder Training Loop\n",
    "\n",
    "for step in tqdm(range(train_steps)):\n",
    "    X_ = next(real_series_iter)\n",
    "    step_e_loss_t0 = train_autoencoder_init(X_)\n",
    "    with writer.as_default():\n",
    "        tf.summary.scalar('Loss Autoencoder Init', step_e_loss_t0, step=step)\n",
    "\n",
    "## Persist model\n",
    "\n",
    "autoencoder.save(log_dir / 'autoencoder')\n",
    "\n",
    "# Phase 2: Supervised training\n",
    "\n",
    "## Define Optimizer\n",
    "\n",
    "supervisor_optimizer = Adam()\n",
    "\n",
    "## Train Step\n",
    "\n",
    "@tf.function\n",
    "def train_supervisor(x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        h = embedder(x)\n",
    "        h_hat_supervised = supervisor(h)\n",
    "        g_loss_s = mse(h[:, 1:, :], h_hat_supervised[:, :-1, :])\n",
    "\n",
    "    var_list = supervisor.trainable_variables\n",
    "    gradients = tape.gradient(g_loss_s, var_list)\n",
    "    supervisor_optimizer.apply_gradients(zip(gradients, var_list))\n",
    "    return g_loss_s\n",
    "\n",
    "## Training Loop\n",
    "\n",
    "for step in tqdm(range(train_steps)):\n",
    "    X_ = next(real_series_iter)\n",
    "    step_g_loss_s = train_supervisor(X_)\n",
    "    with writer.as_default():\n",
    "        tf.summary.scalar('Loss Generator Supervised Init', step_g_loss_s, step=step)\n",
    "\n",
    "## Persist Model\n",
    "\n",
    "supervisor.save(log_dir / 'supervisor')\n",
    "\n",
    "# Joint Training\n",
    "\n",
    "## Generator\n",
    "\n",
    "### Adversarial Architecture - Supervised\n",
    "\n",
    "E_hat = generator(Z)\n",
    "H_hat = supervisor(E_hat)\n",
    "Y_fake = discriminator(H_hat)\n",
    "\n",
    "adversarial_supervised = Model(inputs=Z,\n",
    "                               outputs=Y_fake,\n",
    "                               name='AdversarialNetSupervised')\n",
    "\n",
    "adversarial_supervised.summary()\n",
    "\n",
    "plot_model(adversarial_supervised, show_shapes=True)\n",
    "\n",
    "### Adversarial Architecture in Latent Space\n",
    "\n",
    "Y_fake_e = discriminator(E_hat)\n",
    "\n",
    "adversarial_emb = Model(inputs=Z,\n",
    "                    outputs=Y_fake_e,\n",
    "                    name='AdversarialNet')\n",
    "\n",
    "adversarial_emb.summary()\n",
    "\n",
    "plot_model(adversarial_emb, show_shapes=True)\n",
    "\n",
    "### Mean & Variance Loss\n",
    "\n",
    "X_hat = recovery(H_hat)\n",
    "synthetic_data = Model(inputs=Z,\n",
    "                       outputs=X_hat,\n",
    "                       name='SyntheticData')\n",
    "\n",
    "synthetic_data.summary()\n",
    "\n",
    "plot_model(synthetic_data, show_shapes=True)\n",
    "\n",
    "def get_generator_moment_loss(y_true, y_pred):\n",
    "    y_true_mean, y_true_var = tf.nn.moments(x=y_true, axes=[0])\n",
    "    y_pred_mean, y_pred_var = tf.nn.moments(x=y_pred, axes=[0])\n",
    "    g_loss_mean = tf.reduce_mean(tf.abs(y_true_mean - y_pred_mean))\n",
    "    g_loss_var = tf.reduce_mean(tf.abs(tf.sqrt(y_true_var + 1e-6) - tf.sqrt(y_pred_var + 1e-6)))\n",
    "    return g_loss_mean + g_loss_var\n",
    "\n",
    "## Discriminator\n",
    "\n",
    "### Architecture: Real Data\n",
    "\n",
    "Y_real = discriminator(H)\n",
    "discriminator_model = Model(inputs=X,\n",
    "                            outputs=Y_real,\n",
    "                            name='DiscriminatorReal')\n",
    "\n",
    "discriminator_model.summary()\n",
    "\n",
    "plot_model(discriminator_model, show_shapes=True)\n",
    "\n",
    "## Optimizers\n",
    "\n",
    "generator_optimizer = Adam()\n",
    "discriminator_optimizer = Adam()\n",
    "embedding_optimizer = Adam()\n",
    "\n",
    "## Generator Train Step\n",
    "\n",
    "@tf.function\n",
    "def train_generator(x, z):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_fake = adversarial_supervised(z)\n",
    "        generator_loss_unsupervised = bce(y_true=tf.ones_like(y_fake),\n",
    "                                          y_pred=y_fake)\n",
    "\n",
    "        y_fake_e = adversarial_emb(z)\n",
    "        generator_loss_unsupervised_e = bce(y_true=tf.ones_like(y_fake_e),\n",
    "                                            y_pred=y_fake_e)\n",
    "        h = embedder(x)\n",
    "        h_hat_supervised = supervisor(h)\n",
    "        generator_loss_supervised = mse(h[:, 1:, :], h_hat_supervised[:, 1:, :])\n",
    "\n",
    "        x_hat = synthetic_data(z)\n",
    "        generator_moment_loss = get_generator_moment_loss(x, x_hat)\n",
    "\n",
    "        generator_loss = (generator_loss_unsupervised +\n",
    "                          generator_loss_unsupervised_e +\n",
    "                          100 * tf.sqrt(generator_loss_supervised) +\n",
    "                          100 * generator_moment_loss)\n",
    "\n",
    "    var_list = generator.trainable_variables + supervisor.trainable_variables\n",
    "    gradients = tape.gradient(generator_loss, var_list)\n",
    "    generator_optimizer.apply_gradients(zip(gradients, var_list))\n",
    "    return generator_loss_unsupervised, generator_loss_supervised, generator_moment_loss\n",
    "\n",
    "## Embedding Train Step\n",
    "\n",
    "@tf.function\n",
    "def train_embedder(x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        h = embedder(x)\n",
    "        h_hat_supervised = supervisor(h)\n",
    "        generator_loss_supervised = mse(h[:, 1:, :], h_hat_supervised[:, 1:, :])\n",
    "\n",
    "        x_tilde = autoencoder(x)\n",
    "        embedding_loss_t0 = mse(x, x_tilde)\n",
    "        e_loss = 10 * tf.sqrt(embedding_loss_t0) + 0.1 * generator_loss_supervised\n",
    "\n",
    "    var_list = embedder.trainable_variables + recovery.trainable_variables\n",
    "    gradients = tape.gradient(e_loss, var_list)\n",
    "    embedding_optimizer.apply_gradients(zip(gradients, var_list))\n",
    "    return tf.sqrt(embedding_loss_t0)\n",
    "\n",
    "## Discriminator Train Step\n",
    "\n",
    "@tf.function\n",
    "def get_discriminator_loss(x, z):\n",
    "    y_real = discriminator_model(x)\n",
    "    discriminator_loss_real = bce(y_true=tf.ones_like(y_real),\n",
    "                                  y_pred=y_real)\n",
    "\n",
    "    y_fake = adversarial_supervised(z)\n",
    "    discriminator_loss_fake = bce(y_true=tf.zeros_like(y_fake),\n",
    "                                  y_pred=y_fake)\n",
    "\n",
    "    y_fake_e = adversarial_emb(z)\n",
    "    discriminator_loss_fake_e = bce(y_true=tf.zeros_like(y_fake_e),\n",
    "                                    y_pred=y_fake_e)\n",
    "    return (discriminator_loss_real +\n",
    "            discriminator_loss_fake +\n",
    "            gamma * discriminator_loss_fake_e)\n",
    "\n",
    "@tf.function\n",
    "def train_discriminator(x, z):\n",
    "    with tf.GradientTape() as tape:\n",
    "        discriminator_loss = get_discriminator_loss(x, z)\n",
    "\n",
    "    var_list = discriminator.trainable_variables\n",
    "    gradients = tape.gradient(discriminator_loss, var_list)\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients, var_list))\n",
    "    return discriminator_loss\n",
    "\n",
    "@tf.function\n",
    "def train_encoder_decoder(real_data):\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoded_data = encoder(real_data, training=True)\n",
    "        reconstructed_data = decoder(encoded_data, training=True)\n",
    "        reconstruction_loss = reconstruction_loss_fn(real_data, reconstructed_data)\n",
    "    gradients = tape.gradient(reconstruction_loss, encoder.trainable_variables + decoder.trainable_variables)\n",
    "    encoder_decoder_optimizer.apply_gradients(zip(gradients, encoder.trainable_variables + decoder.trainable_variables))\n",
    "    return reconstruction_loss\n",
    "\n",
    "## Training Loop\n",
    "\n",
    "step_g_loss_u = step_g_loss_s = step_g_loss_v = step_e_loss_t0 = step_d_loss = step_ed_loss = 0\n",
    "\n",
    "# Define loss function and optimizer for the encoder-decoder training\n",
    "reconstruction_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "encoder_decoder_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "for step in range(train_steps):\n",
    "    # Train generator (twice as often as discriminator)\n",
    "    for kk in range(2):\n",
    "        X_ = next(real_series_iter)\n",
    "        Z_ = next(random_series)\n",
    "\n",
    "        # Train generator\n",
    "        step_g_loss_u, step_g_loss_s, step_g_loss_v = train_generator(X_, Z_)\n",
    "        # Train embedder\n",
    "        step_e_loss_t0 = train_embedder(X_)\n",
    "        # Train encoder-decoder\n",
    "        step_ed_loss = train_encoder_decoder(X_)\n",
    "\n",
    "    X_ = next(real_series_iter)\n",
    "    Z_ = next(random_series)\n",
    "    step_d_loss = get_discriminator_loss(X_, Z_)\n",
    "    if step_d_loss > 0.15:\n",
    "        step_d_loss = train_discriminator(X_, Z_)\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "        print(f'{step:6,.0f} | d_loss: {step_d_loss:6.4f} | g_loss_u: {step_g_loss_u:6.4f} | '\n",
    "              f'g_loss_s: {step_g_loss_s:6.4f} | g_loss_v: {step_g_loss_v:6.4f} | e_loss_t0: {step_e_loss_t0:6.4f} | '\n",
    "              f'ed_loss: {step_ed_loss:6.4f}')\n",
    "\n",
    "    with writer.as_default():\n",
    "        tf.summary.scalar('G Loss S', step_g_loss_s, step=step)\n",
    "        tf.summary.scalar('G Loss U', step_g_loss_u, step=step)\n",
    "        tf.summary.scalar('G Loss V', step_g_loss_v, step=step)\n",
    "        tf.summary.scalar('E Loss T0', step_e_loss_t0, step=step)\n",
    "        tf.summary.scalar('D Loss', step_d_loss, step=step)\n",
    "        tf.summary.scalar('ED Loss', step_ed_loss, step=step)\n",
    "\n",
    "# Assuming scaled_data contains the entire dataset\n",
    "data_sequences = []\n",
    "for i in range(len(scaled_data) - seq_len):\n",
    "    data_sequences.append(scaled_data[i:i + seq_len])\n",
    "data_sequences = np.array(data_sequences)\n",
    "\n",
    "data_sequences_reshaped = data_sequences.reshape(-1, seq_len, n_seq)\n",
    "\n",
    "def detect_anomalies(data, encoder, decoder, threshold):\n",
    "    encoded_data = encoder(data, training=False)\n",
    "    reconstructed_data = decoder(encoded_data, training=False)\n",
    "    reconstruction_error = tf.reduce_mean(tf.square(data - reconstructed_data), axis=[1, 2])\n",
    "    anomalies = reconstruction_error > threshold\n",
    "    return anomalies, reconstruction_error\n",
    "\n",
    "threshold = 0.015  # Set an appropriate threshold based on validation data\n",
    "anomalies, reconstruction_errors = detect_anomalies(data_sequences_reshaped, encoder, decoder, threshold)\n",
    "\n",
    "## Persist Synthetic Data Generator\n",
    "\n",
    "synthetic_data.save(log_dir / 'synthetic_data')\n",
    "\n",
    "# Generate Synthetic Data\n",
    "\n",
    "generated_data_scaled = []\n",
    "for i in range(int(n_windows / batch_size)):\n",
    "    Z_ = next(random_series)\n",
    "    d = synthetic_data(Z_)\n",
    "    generated_data_scaled.append(d)\n",
    "\n",
    "generated_data_scaled = np.array(np.vstack(generated_data_scaled))\n",
    "generated_data_scaled.shape\n",
    "\n",
    "def filter_data(generated_data_batches, encoder, decoder, threshold):\n",
    "    improved_generated_data = []\n",
    "    anomalous_data = []\n",
    "    for batch in generated_data_batches:\n",
    "        for sample in batch:\n",
    "            # Ensure sample has the correct shape\n",
    "            sample = tf.reshape(sample, [1, sample.shape[0], sample.shape[1]])  # Reshape to (1, 24, 1)\n",
    "            \n",
    "            # Encode and decode the sample\n",
    "            encoded_sample = encoder(sample, training=False)\n",
    "            reconstructed_sample = decoder(encoded_sample, training=False)\n",
    "            \n",
    "            # Calculate reconstruction error\n",
    "            reconstruction_error = tf.reduce_mean(tf.square(sample - reconstructed_sample))\n",
    "            \n",
    "            # Filter out anomalies based on reconstruction error\n",
    "            if reconstruction_error < threshold:\n",
    "                improved_generated_data.append(sample)\n",
    "            else:\n",
    "                anomalous_data.append(sample)\n",
    "                \n",
    "    improved_generated_data = np.array(improved_generated_data)\n",
    "    \n",
    "    # Reshape back to original sample shape (24, 1)\n",
    "    improved_generated_data = np.reshape(improved_generated_data, (-1, 24, 2))\n",
    "    \n",
    "    return improved_generated_data\n",
    "\n",
    "threshold = 0.015  # Set an appropriate threshold based on validation data\n",
    "\n",
    "# Assuming generated_data_scaled is a batch, so we need to convert it into a list of batches\n",
    "generated_data_batches = [generated_data_scaled]\n",
    "\n",
    "improved_generated_data = filter_data(generated_data_batches, encoder, decoder, threshold)\n",
    "print(improved_generated_data.shape)\n",
    "\n",
    "len(generated_data_scaled)\n",
    "\n",
    "np.save(log_dir / 'generated_data.npy', generated_data_scaled)\n",
    "\n",
    "## Rescale\n",
    "\n",
    "generated_data = (scaler.inverse_transform(generated_data_scaled\n",
    "                                           .reshape(-1, n_seq))\n",
    "                  .reshape(-1, seq_len, n_seq))\n",
    "print(generated_data.shape)\n",
    "generated_ano_data = (scaler.inverse_transform(improved_generated_data\n",
    "                                           .reshape(-1, n_seq))\n",
    "                  .reshape(-1, seq_len, n_seq))\n",
    "print(generated_ano_data.shape)\n",
    "\n",
    "differences = generated_data != generated_ano_data\n",
    "indices = np.where(differences)\n",
    "print(indices)\n",
    "\n",
    "## Persist Data\n",
    "\n",
    "with pd.HDFStore(hdf_store) as store:\n",
    "    store.put('data/synthetic', pd.DataFrame(generated_data.reshape(-1, n_seq),\n",
    "                                             columns=['WTI','BRENT']))\n",
    "\n",
    "## Plot sample Series\n",
    "\n",
    "# Generate synthetic data for a random window\n",
    "rand = np.random.randint(len(generated_data))\n",
    "synthetic = generated_data[rand]\n",
    "\n",
    "# Select a random real data window\n",
    "idx = np.random.randint(len(df) - seq_len)\n",
    "real = df.iloc[idx: idx + seq_len]\n",
    "\n",
    "# Create a DataFrame with both real and synthetic data\n",
    "data_to_plot = pd.DataFrame({\n",
    "    'Real': real['WTI'].values,\n",
    "    'Synthetic': synthetic[:, 0]  # Assuming synthetic data has the same shape\n",
    "})\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "data_to_plot.plot(ax=ax, title='Close', secondary_y='Synthetic', style=['-', '--'], lw=1)\n",
    "sns.despine()\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Generate synthetic data for a random window\n",
    "rand = np.random.randint(len(generated_data))\n",
    "synthetic = generated_data[rand]\n",
    "\n",
    "# Select a random real data window\n",
    "idx = np.random.randint(len(df) - seq_len)\n",
    "real = df.iloc[idx: idx + seq_len]\n",
    "\n",
    "# Create a DataFrame with both real and synthetic data\n",
    "data_to_plot = pd.DataFrame({\n",
    "    'Real': real['BRENT'].values,\n",
    "    'Synthetic': synthetic[:, 0]  # Assuming synthetic data has the same shape\n",
    "})\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "data_to_plot.plot(ax=ax, title='Close', secondary_y='Synthetic', style=['-', '--'], lw=1)\n",
    "sns.despine()\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Generate synthetic data for the same random window\n",
    "synthetic_ano_data = generated_ano_data[rand]\n",
    "\n",
    "# Select a random real data window\n",
    "real = df.iloc[idx: idx + seq_len]\n",
    "\n",
    "# Create a DataFrame with both real and synthetic data\n",
    "data_to_plot = pd.DataFrame({\n",
    "    'Real': real['WTI'].values,\n",
    "    'Synthetic Improved': synthetic_ano_data[:, 0]  # Assuming synthetic data has the same shape\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "data_to_plot.plot(ax=ax, title='Close', secondary_y='Synthetic Improved', style=['-', '--'], lw=1)\n",
    "sns.despine()\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "experiment = 0\n",
    "\n",
    "df.to_hdf(hdf_store, 'data/real')\n",
    "df\n",
    "\n",
    "def get_real_data():\n",
    "    df = pd.read_hdf(hdf_store, 'data/real').sort_index()\n",
    "\n",
    "    # Preprocess the dataset:\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "    data = []\n",
    "    for i in range(len(df) - seq_len):\n",
    "        data.append(scaled_data[i:i + seq_len])\n",
    "    return data\n",
    "\n",
    "\n",
    "real_data = get_real_data()\n",
    "\n",
    "n = len(real_data)\n",
    "n\n",
    "\n",
    "np.asarray(real_data).shape\n",
    "\n",
    "synthetic_data = improved_generated_data\n",
    "synthetic_data.shape\n",
    "\n",
    "real_data = real_data[:synthetic_data.shape[0]]\n",
    "len(real_data)\n",
    "\n",
    "sample_size = 250\n",
    "idx = np.random.permutation(len(real_data))[:sample_size]\n",
    "\n",
    "# Data preprocessing\n",
    "real_sample = np.asarray(real_data)[idx]\n",
    "synthetic_sample = np.asarray(synthetic_data)[idx]\n",
    "\n",
    "real_sample_2d = real_sample.reshape(-1, seq_len)\n",
    "synthetic_sample_2d = synthetic_sample.reshape(-1, seq_len)\n",
    "\n",
    "real_sample_2d.shape, synthetic_sample_2d.shape\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(real_sample_2d)\n",
    "\n",
    "# Transform the data and create DataFrames with an additional 'Data' column\n",
    "pca_real = (pd.DataFrame(pca.transform(real_sample_2d))\n",
    "            .assign(Data='Real'))\n",
    "pca_synthetic = (pd.DataFrame(pca.transform(synthetic_sample_2d))\n",
    "                 .assign(Data='Synthetic'))\n",
    "\n",
    "# Concatenate the DataFrames and rename the columns\n",
    "pca_result = pd.concat([pca_real, pca_synthetic]).rename(\n",
    "    columns={0: '1st Component', 1: '2nd Component'})\n",
    "\n",
    "print(pca_result)\n",
    "\n",
    "\n",
    "tsne_data = np.concatenate((real_sample_2d,\n",
    "                            synthetic_sample_2d), axis=0)\n",
    "\n",
    "tsne = TSNE(n_components=2,\n",
    "            verbose=1,\n",
    "            perplexity=200)\n",
    "tsne_result = tsne.fit_transform(tsne_data)\n",
    "\n",
    "tsne_result = pd.DataFrame(tsne_result, columns=['X', 'Y']).assign(Data='Real')\n",
    "tsne_result.loc[sample_size*1.5:, 'Data'] = 'Synthetic'\n",
    "\n",
    "pca_result\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(14, 5))\n",
    "\n",
    "# PCA Result\n",
    "sns.scatterplot(x='1st Component', y='2nd Component', data=pca_result,\n",
    "                hue='Data', style='Data', ax=axes[0])\n",
    "sns.despine()\n",
    "axes[0].set_title('PCA Result')\n",
    "\n",
    "# t-SNE Result\n",
    "sns.scatterplot(x='X', y='Y', data=tsne_result,\n",
    "                hue='Data', style='Data', ax=axes[1])\n",
    "sns.despine()\n",
    "axes[1].set_title('t-SNE Result')\n",
    "\n",
    "# Set overall title and layout adjustments\n",
    "fig.suptitle('Assessing Diversity: Qualitative Comparison of Real and Synthetic Data Distributions', \n",
    "             fontsize=14)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=.88)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "real_data = get_real_data()\n",
    "real_data = np.array(real_data)[:len(synthetic_data)]\n",
    "\n",
    "synthetic_data.shape\n",
    "\n",
    "n_series = real_data.shape[0]\n",
    "\n",
    "idx = np.arange(n_series)\n",
    "\n",
    "n_train = int(.8*n_series)\n",
    "train_idx = idx[:n_train]\n",
    "test_idx = idx[n_train:]\n",
    "\n",
    "train_data = np.vstack((real_data[train_idx], \n",
    "                        synthetic_data[train_idx]))\n",
    "test_data = np.vstack((real_data[test_idx], \n",
    "                       synthetic_data[test_idx]))\n",
    "\n",
    "n_train, n_test = len(train_idx), len(test_idx)\n",
    "train_labels = np.concatenate((np.ones(n_train),\n",
    "                               np.zeros(n_train)))\n",
    "test_labels = np.concatenate((np.ones(n_test),\n",
    "                              np.zeros(n_test)))\n",
    "\n",
    "ts_classifier = Sequential([GRU(2, input_shape=(24, 2), name='GRU'),\n",
    "                            Dense(1, activation='sigmoid', name='OUT')],\n",
    "                           name='Time_Series_Classifier')\n",
    "\n",
    "ts_classifier.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=[AUC(name='AUC'), 'accuracy'])\n",
    "\n",
    "ts_classifier.summary()\n",
    "\n",
    "test_data\n",
    "\n",
    "# Now, train your model with the modified input data\n",
    "result = ts_classifier.fit(x=train_data,\n",
    "                            y=train_labels,\n",
    "                            validation_data=(test_data, test_labels),\n",
    "                            epochs=250,\n",
    "                            batch_size=128,\n",
    "                            verbose=0)\n",
    "\n",
    "ts_classifier.evaluate(x=test_data, y=test_labels)\n",
    "\n",
    "history = pd.DataFrame(result.history)\n",
    "history.info()\n",
    "\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "sns.set_style('white')\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(14,4))\n",
    "history[['AUC', 'val_AUC']].rename(columns={'AUC': 'Train', 'val_AUC': 'Test'}).plot(ax=axes[1], \n",
    "                                                                                     title='ROC Area under the Curve',\n",
    "                                                                                    style=['-', '--'],\n",
    "                                                                                    xlim=(0, 250))\n",
    "history[['accuracy', 'val_accuracy']].rename(columns={'accuracy': 'Train', 'val_accuracy': 'Test'}).plot(ax=axes[0], \n",
    "                                                                                                         title='Accuracy',\n",
    "                                                                                                        style=['-', '--'],\n",
    "                                                                                                        xlim=(0, 250))\n",
    "for i in [0, 1]:\n",
    "    axes[i].set_xlabel('Epoch')\n",
    "\n",
    "axes[0].yaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y))) \n",
    "axes[0].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_ylabel('AUC')\n",
    "sns.despine()\n",
    "fig.suptitle('Assessing Fidelity: Time Series Classification Performance', fontsize=14)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=.85);\n",
    "\n",
    "real_data = get_real_data()\n",
    "real_data = np.array(real_data)[:len(synthetic_data)]\n",
    "\n",
    "real_data.shape, synthetic_data.shape\n",
    "\n",
    "real_train_data = real_data[train_idx, :23, :]\n",
    "real_train_label = real_data[train_idx, -1, :]\n",
    "\n",
    "real_test_data = real_data[test_idx, :23, :]\n",
    "real_test_label = real_data[test_idx, -1, :]\n",
    "\n",
    "real_train_data.shape, real_train_label.shape, real_test_data.shape, real_test_label.shape\n",
    "\n",
    "synthetic_train = synthetic_data[:, :23, :]\n",
    "synthetic_label = synthetic_data[:, -1, :]\n",
    "\n",
    "synthetic_train.shape, synthetic_label.shape\n",
    "\n",
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(23, 2), return_sequences=False))\n",
    "    model.add(Dense(2))\n",
    "    model.compile(optimizer='adam', loss='mean_absolute_error')\n",
    "    return model\n",
    "\n",
    "ts_regression = get_model()\n",
    "real_result = ts_regression.fit(x=real_train_data,\n",
    "                                y=real_train_label,\n",
    "                                validation_data=(\n",
    "                                    real_test_data, \n",
    "                                    real_test_label),\n",
    "                                epochs=100,\n",
    "                                batch_size=128,\n",
    "                                verbose=0)\n",
    "\n",
    "ts_regression = get_model()\n",
    "synthetic_result = ts_regression.fit(x=synthetic_train,\n",
    "                                     y=synthetic_label,\n",
    "                                     validation_data=(\n",
    "                                         real_test_data, \n",
    "                                         real_test_label),\n",
    "                                     epochs=100,\n",
    "                                     batch_size=128,\n",
    "                                     verbose=0)\n",
    "\n",
    "synthetic_result = pd.DataFrame(synthetic_result.history).rename(columns={'loss': 'Train', 'val_loss': 'Test'})\n",
    "real_result = pd.DataFrame(real_result.history).rename(columns={'loss': 'Train', 'val_loss': 'Test'})\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(14, 4), sharey=True)\n",
    "synthetic_result.plot(ax=axes[0], title='Train on Synthetic, Test on Real', logy=True, xlim=(0, 100))\n",
    "real_result.plot(ax=axes[1], title='Train on Real, Test on Real', logy=True, xlim=(0, 100))\n",
    "for i in [0, 1]:\n",
    "    axes[i].set_xlabel('Epoch')\n",
    "    axes[i].set_ylabel('Mean Absolute Error (log scale)')\n",
    "\n",
    "sns.despine()\n",
    "fig.suptitle('Assessing Usefulness: Time Series Prediction Performance', fontsize=14)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=.85);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
