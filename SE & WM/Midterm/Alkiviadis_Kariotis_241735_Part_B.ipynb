{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce411019-8d1d-420a-8bb8-e72ddee1911f",
   "metadata": {},
   "source": [
    "# Part B: Take Home | Alkiviadis Kariotis 241735"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dee5b26-e774-4172-a68f-6e26c25ae9db",
   "metadata": {},
   "source": [
    "## Use your favorite web crawling technique and crawl information related to the CORONAVIRUS from the following websites:\n",
    "https://www.who.int/emergencies/diseases/novel-coronavirus-2019\n",
    "https://www.un.org/en/coronavirus\n",
    "\n",
    "Make sure that you just get:\n",
    "-   information in English,\n",
    "-   information RELATED to coronavirus / COVID-19 (including non-textual data if any),\n",
    "-   information on the topic as found in the above pages and the links included in these pages\n",
    "\n",
    "\n",
    "Once you have finished, answer the following questions:\n",
    "*     How many documents have you successfully acquired? [5%]\n",
    "*     How did you restrict or clean-up your crawling as requested above? Please paste in this space your exact code and corresponding explanation of how the crawler was restricted as requested; pay particular notice at justifying the necessity of your decisions (e.g., you should not include restrictions that are redundant/have no effect in crawling because default restrictions apply, etc.). [20%]\n",
    "*     Vectorize the MAIN text contained in the following link using Vector Space approach? Use a vocabulary of a maximum of 20 words that you choose carefully to be representative of the document. State the criteria that made you make these choices. [25%] https://www.who.int/health-topics/coronavirus#tab=tab 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce4602ab-dd3e-43f9-b4b5-35dc5034fad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieved content from https://www.who.int/emergencies/diseases/novel-coronavirus-2019\n",
      "Successfully retrieved content from https://www.un.org/en/coronavirus\n",
      "Successfully retrieved content from https://news.un.org/en/tags/covid-19\n",
      "Successfully retrieved content from https://news.un.org/en/events/un-news-coverage-coronavirus-outbreak\n",
      "Successfully retrieved content from https://www.un.org/en/coronavirus/covid-19-faqs\n",
      "Successfully retrieved content from https://www.un.org/en/coronavirus/covid-19-faqs\n",
      "Successfully retrieved content from https://www.un.org/en/coronavirus/covid-19-faqs\n",
      "Successfully retrieved content from https://www.un.org/en/coronavirus/covid-19-faqs\n",
      "Successfully retrieved content from https://www.un.org/en/coronavirus/financing-development/global-accelerator\n",
      "Successfully retrieved content from https://www.un.org/en/coronavirus/information-un-system\n",
      "Successfully retrieved content from https://www.un.org/en/coronavirus/information-un-system\n",
      "Successfully retrieved content from https://www.un.org/en/coronavirus/information-un-system\n",
      "Successfully retrieved content from https://www.un.org/en/coronavirus/information-un-system\n",
      "Successfully retrieved content from https://www.un.org/en/coronavirus/staff\n",
      "Successfully retrieved content from https://www.un.org/en/coronavirus/staff\n",
      "Successfully retrieved content from https://www.un.org/en/coronavirus/member-states#resolutions\n",
      "Successfully retrieved content from https://www.un.org/en/coronavirus/un-secretary-general#reports\n",
      "Successfully retrieved content from https://media.un.org/en/search/?q=covid-19&filter-topic=CORONAVIRUS+DISEASE+-+COVID+19&sort-by=date_desc\n",
      "Successfully retrieved content from https://www.un.org/en/coronavirus/social-media-posters\n",
      "Successfully retrieved content from https://www.un.org/en/coronavirus/communication-resources\n",
      "Successfully retrieved content from https://www.un.org/en/coronavirus/be-ready-covid-19-key-scenarios\n",
      "UN site: Retrieved 19 documents.\n",
      "Example content from UN site:\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\" dir=\"ltr\" prefix=\"og: https://ogp.me/ns#\">\n",
      "  <head>\n",
      "    <meta charset=\"utf-8\" />\n",
      "<script>window.dataLayer = window.dataLayer || []; window.dataLayer.push({\"drupalLanguage\":\"en\",\"drupalCountry\":\"US\",\"siteName\":\"UN News\",\"entityLangcode\":\"en\",\"entityType\":\"taxonomy_term\",\"entityBundle\":\"tags\",\"entityId\":\"233091\",\"entityTitle\":\"COVID-19\",\"userUid\":0});</script>\n",
      "<link rel=\"canonical\" href=\"https://news.un.org/en/tags/covid-19\" />\n",
      "<link rel=\"shortlink\" href=\"https://ne\n",
      "WHO site: Retrieved 0 documents.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "# Function to parse HTML content and extract links\n",
    "def extract_links(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    links = [link['href'] for link in soup.find_all('a', href=True) if 'http' in link['href']]\n",
    "    return links\n",
    "\n",
    "# Filter functions\n",
    "def is_english_url(url):\n",
    "    return '/en/' in url or 'lang=en' in url or re.search(r'english|en_', url, re.IGNORECASE) is not None\n",
    "\n",
    "def is_related_to_covid(url):\n",
    "    covid_keywords = ['coronavirus', 'covid', 'covid-19', 'pandemic']\n",
    "    return any(keyword in url.lower() for keyword in covid_keywords)\n",
    "\n",
    "# Constants\n",
    "USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "DELAY_BETWEEN_REQUESTS = 2  # Delay in seconds\n",
    "\n",
    "# Function to crawl a website and extract COVID-related information\n",
    "def crawl_website(url):\n",
    "    headers = {'User-Agent': USER_AGENT}\n",
    "    try:\n",
    "        time.sleep(DELAY_BETWEEN_REQUESTS)  # Delay to prevent being blocked\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            page_content = response.text\n",
    "            print(f\"Successfully retrieved content from {url}\")\n",
    "            return page_content\n",
    "        else:\n",
    "            print(f\"Failed to retrieve content from {url}, status code: {response.status_code}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to run the crawler and store contents\n",
    "def run_crawler_and_store_contents(urls):\n",
    "    contents = {\n",
    "        'UN': [],\n",
    "        'WHO': []\n",
    "    }\n",
    "    for url in urls:\n",
    "        page_content = crawl_website(url)\n",
    "        if page_content:\n",
    "            links = extract_links(page_content)\n",
    "            filtered_links = [link for link in links if is_english_url(link) and is_related_to_covid(link)]\n",
    "            for link in filtered_links:\n",
    "                subpage_content = crawl_website(link)\n",
    "                if subpage_content:\n",
    "                    if 'un.org' in link:\n",
    "                        contents['UN'].append(subpage_content)\n",
    "                    elif 'who.int' in link:\n",
    "                        contents['WHO'].append(subpage_content)\n",
    "    return contents\n",
    "\n",
    "# Function to print the summary of contents\n",
    "def print_summary(contents):\n",
    "    for site, pages in contents.items():\n",
    "        print(f\"{site} site: Retrieved {len(pages)} documents.\")\n",
    "        if pages:  # If there is at least one document\n",
    "            # Print first 500 characters for example\n",
    "            print(f\"Example content from {site} site:\\n{pages[0][:500]}\")\n",
    "\n",
    "# List of URLs to crawl\n",
    "urls = [\n",
    "    'https://www.who.int/emergencies/diseases/novel-coronavirus-2019',\n",
    "    'https://www.un.org/en/coronavirus'\n",
    "]\n",
    "\n",
    "# Run the crawler and store contents\n",
    "contents = run_crawler_and_store_contents(urls)\n",
    "\n",
    "# Print the summary of retrieved contents\n",
    "print_summary(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd559b85-6e2c-411d-a36f-42561723d338",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c15436-ddb4-471b-8c20-beaa45ad424a",
   "metadata": {},
   "source": [
    "## Documents successfully acquired (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a03d24be-ddc6-4b20-9cc3-3e0b8148c5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UN site: Retrieved 19 documents.\n",
      "Example content from UN site:\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\" dir=\"ltr\" prefix=\"og: https://ogp.me/ns#\">\n",
      "  <head>\n",
      "    <meta charset=\"utf-8\" />\n",
      "<script>window.dataLayer = window.dataLayer || []; window.dataLayer.push({\"drupalLanguage\":\"en\",\"drupalCountry\":\"US\",\"siteName\":\"UN News\",\"entityLangcode\":\"en\",\"entityType\":\"taxonomy_term\",\"entityBundle\":\"tags\",\"entityId\":\"233091\",\"entityTitle\":\"COVID-19\",\"userUid\":0});</script>\n",
      "<link rel=\"canonical\" href=\"https://news.un.org/en/tags/covid-19\" />\n",
      "<link rel=\"shortlink\" href=\"https://ne\n",
      "WHO site: Retrieved 0 documents.\n"
     ]
    }
   ],
   "source": [
    "print_summary(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aa244f-f968-4e64-a593-ed4ec9f249ce",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080d9a10-0244-43f0-9e5b-1d54837b4371",
   "metadata": {},
   "source": [
    "## Restricting and Cleaning-up the Crawling: (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7f12567-0bb2-4794-9ba0-49a5280d137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Language Restriction:\n",
    "def is_english_url(url):\n",
    "    return '/en/' in url or 'lang=en' in url or re.search(r'english|en_', url, re.IGNORECASE) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5151e996-170e-4ee0-8147-695b5eb4c410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Topic Filtering\n",
    "def is_related_to_covid(url):\n",
    "    covid_keywords = ['coronavirus', 'covid', 'covid-19', 'pandemic']\n",
    "    return any(keyword in url.lower() for keyword in covid_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "395c8ac2-f114-4442-abdc-3a5e6f095b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#User-Agent and Delays\n",
    "headers = {'User-Agent': USER_AGENT}\n",
    "time.sleep(DELAY_BETWEEN_REQUESTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c960b6-d9e7-42c9-852c-f6a3b69e329b",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fe42bc-3137-4f02-9f06-3ea91c9ef943",
   "metadata": {},
   "source": [
    "## Vectorizing the MAIN text:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53ccac5-8c97-45aa-a982-88e12536e144",
   "metadata": {},
   "source": [
    "### First attempt without the stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0c7f4af-560b-4456-8aa8-ae0ad62f5352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['and', 'the', 'to', 'of', 'a', 'in', 'or', 'COVID-19', 'is', 'with', 'at', 'from', 'on', 'disease', 'by', 'people', 'infected', 'virus', 'respiratory', 'for']\n",
      "Vector representation: [[27 22 16  9  0  7  6  0  5  5  5  5  6  6  4  5  4  5  4  4]]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "#Retrieve HTML content\n",
    "def get_html(url):\n",
    "    response = requests.get(url)\n",
    "    return response.text if response.status_code == 200 else None\n",
    "\n",
    "#Parse HTML to extract the main text\n",
    "def extract_main_text(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    article_text = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
    "    return article_text\n",
    "\n",
    "#Select a representative vocabulary\n",
    "def select_vocabulary(text, max_features=20):\n",
    "    # Tokenize the text\n",
    "    tokens = text.split()\n",
    "    # Count the frequency of each token\n",
    "    counter = Counter(tokens)\n",
    "    # Select the most common tokens as the vocabulary\n",
    "    most_common_tokens = [word for word, count in counter.most_common(max_features)]\n",
    "    return most_common_tokens\n",
    "\n",
    "#Create a document-term matrix or vector for the main text\n",
    "def vectorize_text(text, vocabulary):\n",
    "    vectorizer = CountVectorizer(vocabulary=vocabulary)\n",
    "    vector = vectorizer.fit_transform([text])\n",
    "    return vector.toarray()\n",
    "\n",
    "\n",
    "url = 'https://www.who.int/health-topics/coronavirus#tab=tab_1'\n",
    "\n",
    "# Perform the vectorization process\n",
    "html_content = get_html(url)\n",
    "if html_content:\n",
    "    main_text = extract_main_text(html_content)\n",
    "    vocabulary = select_vocabulary(main_text)\n",
    "    vector = vectorize_text(main_text, vocabulary)\n",
    "    print(\"Vocabulary:\", vocabulary)\n",
    "    print(\"Vector representation:\", vector)\n",
    "else:\n",
    "    print(\"Failed to retrieve HTML content\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ac4e8f-b216-4522-bae9-c8f8e0d7cc56",
   "metadata": {},
   "source": [
    "###  To make the vocabulary more representative of the document, we can consider the following:\n",
    "\n",
    "1.Synonyms and Variations\n",
    "2.Medical and Scientific Terms\n",
    "3.Public Health Keywords\n",
    "4.Severity and Impact\n",
    "5.Action Words\n",
    "6.Policy and Guidance\n",
    "\n",
    "We could refine our vocabulary by replacing less informative words with terms that could add more specificity and relevance to the document's subject, based on the aformentioned criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aba57b70-7e3f-45c5-a8e0-7aed8b450356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['COVID-19', 'disease', 'people', 'infected', 'virus', 'respiratory', 'health', 'mild', 'recover', 'medical', 'symptoms:', 'symptoms', 'Health', 'pandemic', 'SARS-CoV-2', 'moderate', 'illness', 'without', 'become', 'seriously']\n",
      "Vector representation: [[0 6 5 4 5 4 8 3 3 3 0 7 0 3 0 2 3 2 2 2]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/alkis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download the set of stop words the first time\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#Retrieve HTML content\n",
    "def get_html(url):\n",
    "    response = requests.get(url)\n",
    "    return response.text if response.status_code == 200 else None\n",
    "\n",
    "#Parse HTML to extract the main text\n",
    "def extract_main_text(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    article_text = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
    "    return article_text\n",
    "\n",
    "#Select a representative vocabulary\n",
    "def select_vocabulary(text, max_features=20):\n",
    "    # Tokenize the text\n",
    "    tokens = text.split()\n",
    "    # Remove common stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    # Count the frequency of each token\n",
    "    counter = Counter(filtered_tokens)\n",
    "    # Select the most common tokens as the vocabulary\n",
    "    most_common_tokens = [word for word, count in counter.most_common(max_features)]\n",
    "    return most_common_tokens\n",
    "\n",
    "#Create a document-term matrix or vector for the main text\n",
    "def vectorize_text(text, vocabulary):\n",
    "    vectorizer = CountVectorizer(vocabulary=vocabulary)\n",
    "    vector = vectorizer.fit_transform([text])\n",
    "    return vector.toarray()\n",
    "\n",
    "\n",
    "url = 'https://www.who.int/health-topics/coronavirus#tab=tab_1'\n",
    "\n",
    "# Perform the vectorization process\n",
    "html_content = get_html(url)\n",
    "if html_content:\n",
    "    main_text = extract_main_text(html_content)\n",
    "    vocabulary = select_vocabulary(main_text)\n",
    "    vector = vectorize_text(main_text, vocabulary)\n",
    "    print(\"Vocabulary:\", vocabulary)\n",
    "    print(\"Vector representation:\", vector)\n",
    "else:\n",
    "    print(\"Failed to retrieve HTML content\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bc491a-4543-4d8f-90fb-3744f8735ce4",
   "metadata": {},
   "source": [
    "### The following includes a predefined list of coronavirus-related terms that are expected to be representative of the content. We then filter the most common words from the text to match this list, ensuring that the final vocabulary is both frequent in the document and specific to the coronavirus topic. The isalpha check ensures that we're only dealing with words (not numbers or symbols), and we use a pool larger than 20 words before the final filtering to ensure we have enough words to choose from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41fc9987-6c29-4476-aa35-40ab6f17d147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/alkis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['symptoms', 'pandemic', 'prevent', 'transmission', 'Coronavirus']\n",
      "Vector representation: [[7 3 2 2 0]]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download the set of stop words the first time\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Retrieve HTML content\n",
    "def get_html(url):\n",
    "    response = requests.get(url)\n",
    "    return response.text if response.status_code == 200 else None\n",
    "\n",
    "# Parse HTML to extract the main text\n",
    "def extract_main_text(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    article_text = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
    "    return article_text\n",
    "\n",
    "# Select a representative vocabulary\n",
    "def select_vocabulary(text, max_features=20):\n",
    "    # Tokenize the text\n",
    "    tokens = text.split()\n",
    "    # Remove common stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word.isalpha()]\n",
    "    # Count the frequency of each token\n",
    "    counter = Counter(filtered_tokens)\n",
    "    # Select the most common tokens as the vocabulary\n",
    "    most_common_tokens = [word for word, count in counter.most_common(max_features * 2)]  # Increase to have a selection pool\n",
    "\n",
    "    # Apply further criteria to refine the vocabulary\n",
    "    covid_related_terms = ['coronavirus', 'COVID-19', 'pandemic', 'SARS-CoV-2', 'transmission', 'quarantine', \n",
    "                           'vaccination', 'symptoms', 'outbreak', 'spread', 'prevent', 'treatment', 'mask', \n",
    "                           'social distancing', 'public health', 'testing', 'immune', 'hospital', 'sanitizer', 'guidelines']\n",
    "    # Intersect the most common with the covid_related list while preserving order\n",
    "    representative_tokens = [word for word in most_common_tokens if word.lower() in covid_related_terms][:max_features]\n",
    "    \n",
    "    return representative_tokens\n",
    "\n",
    "# Create a document-term matrix or vector for the main text\n",
    "def vectorize_text(text, vocabulary):\n",
    "    vectorizer = CountVectorizer(vocabulary=vocabulary)\n",
    "    vector = vectorizer.fit_transform([text])\n",
    "    return vector.toarray()\n",
    "\n",
    "url = 'https://www.who.int/health-topics/coronavirus#tab=tab_1'\n",
    "\n",
    "# Perform the vectorization process\n",
    "html_content = get_html(url)\n",
    "if html_content:\n",
    "    main_text = extract_main_text(html_content)\n",
    "    vocabulary = select_vocabulary(main_text)\n",
    "    vector = vectorize_text(main_text, vocabulary)\n",
    "    print(\"Vocabulary:\", vocabulary)\n",
    "    print(\"Vector representation:\", vector)\n",
    "else:\n",
    "    print(\"Failed to retrieve HTML content\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
